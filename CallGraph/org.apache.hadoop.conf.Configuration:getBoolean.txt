org.apache.hadoop.ipc.WritableRpcEngine:getServer
org.apache.hadoop.ipc.ProtobufRpcEngine:getServer
org.apache.hadoop.ipc.WritableRpcEngine$Server:<init>
org.apache.hadoop.ipc.ProtobufRpcEngine$Server:<init>
org.apache.hadoop.ipc.RPC$Server:<init>
org.apache.hadoop.io.retry.RetryUtils:getDefaultRetryPolicy
org.apache.hadoop.util.NativeLibraryChecker:main
org.apache.hadoop.io.compress.BZip2Codec:createCompressor
org.apache.hadoop.io.compress.BZip2Codec:getDecompressorType
org.apache.hadoop.io.compress.BZip2Codec:createInputStream
org.apache.hadoop.io.compress.BZip2Codec:createDecompressor
org.apache.hadoop.io.compress.BZip2Codec:getCompressorType
org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getLibraryName
org.apache.hadoop.io.compress.BZip2Codec:createOutputStream
org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getBzip2Compressor
org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getBzip2DecompressorType
org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getBzip2Decompressor
org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getBzip2CompressorType
org.apache.hadoop.io.BloomMapFile$Reader:get
org.apache.hadoop.io.SetFile$Reader:get
org.apache.hadoop.io.MapFile$Reader:get
org.apache.hadoop.io.SetFile$Reader:seek
org.apache.hadoop.io.MapFile$Reader:seek
org.apache.hadoop.io.MapFile$Reader:getClosest
org.apache.hadoop.io.MapFile:main
org.apache.hadoop.io.MapFile$Reader:midKey
org.apache.hadoop.io.MapFile$Reader:finalKey
org.apache.hadoop.io.MapFile$Reader:seekInternal
org.apache.hadoop.fs.shell.Display$TextRecordInputStream:read
org.apache.hadoop.io.MapFile$Reader:next
org.apache.hadoop.io.MapFile:fix
org.apache.hadoop.io.MapFile$Reader:readIndex
org.apache.hadoop.io.SequenceFile$Reader:next
org.apache.hadoop.io.SequenceFile$Reader:sync
org.apache.hadoop.ha.ZKFailoverController$1:run
org.apache.hadoop.ha.ZKFailoverController:access$000
org.apache.hadoop.ha.ZKFailoverController:doRun
org.apache.hadoop.ha.ZKFailoverController:initRPC
org.apache.hadoop.io.SetFile$Writer:<init>
org.apache.hadoop.io.BloomMapFile$Writer:<init>
org.apache.hadoop.io.ArrayFile$Writer:<init>
org.apache.hadoop.io.BloomMapFile$Reader:<init>
org.apache.hadoop.ha.ZKFCRpcServer:gracefulFailover
org.apache.hadoop.ha.ZKFailoverController:run
org.apache.hadoop.ha.ZKFCRpcServer:cedeActive
org.apache.hadoop.ipc.Client$Connection:access$1500
org.apache.hadoop.fs.FileSystem:newInstanceLocal
org.apache.hadoop.fs.FileSystem$2:run
org.apache.hadoop.security.UserGroupInformation:getUGIFromTicketCache
org.apache.hadoop.fs.FileSystem:newInstance
org.apache.hadoop.fs.viewfs.ViewFs:<init>
org.apache.hadoop.fs.viewfs.ViewFileSystem:initialize
org.apache.hadoop.fs.viewfs.ViewFs$1:<init>
org.apache.hadoop.fs.viewfs.ViewFileSystem$1:<init>
org.apache.hadoop.ipc.Server$Listener$Reader:run
org.apache.hadoop.ipc.Server$Listener$Reader:doRunLoop
org.apache.hadoop.ipc.Server$Listener:doRead
org.apache.hadoop.ipc.Server$Connection:readAndProcess
org.apache.hadoop.ipc.Server$Connection:unwrapPacketAndProcessRpcs
org.apache.hadoop.ipc.Server$Connection:processOneRpc
org.apache.hadoop.ipc.Server$Connection:processRpcOutOfBandRequest
org.apache.hadoop.ipc.Server$Connection:saslReadAndProcess
org.apache.hadoop.ipc.Server$Connection:saslProcess
org.apache.hadoop.ipc.Server$Connection:processSaslMessage
org.apache.hadoop.ipc.Server$Connection:buildSaslNegotiateResponse
org.apache.hadoop.ipc.Server$Connection:createSaslServer
org.apache.hadoop.ipc.Server:buildNegotiateResponse
org.apache.hadoop.ipc.RPC:waitForProxy
org.apache.hadoop.ha.HAAdmin:run
org.apache.hadoop.ha.HAAdmin:runCmd
org.apache.hadoop.ha.HAAdmin:failover
org.apache.hadoop.ha.FailoverController:failover
org.apache.hadoop.ha.HealthMonitor$MonitorDaemon:run
org.apache.hadoop.ha.HealthMonitor:access$500
org.apache.hadoop.ha.HealthMonitor:loopUntilConnected
org.apache.hadoop.ha.HealthMonitor:tryConnect
org.apache.hadoop.ha.ZKFailoverController$2:run
org.apache.hadoop.ha.ZKFailoverController:access$300
org.apache.hadoop.ha.ZKFailoverController$ElectorCallbacks:becomeActive
org.apache.hadoop.ha.ZKFailoverController:access$900
org.apache.hadoop.ha.ZKFailoverController$ElectorCallbacks:becomeStandby
org.apache.hadoop.ha.ZKFailoverController:access$1000
org.apache.hadoop.ha.ZKFailoverController$ElectorCallbacks:fenceOldActive
org.apache.hadoop.ha.ZKFailoverController:access$1100
org.apache.hadoop.ha.ZKFailoverController:fenceOldActive
org.apache.hadoop.ha.ZKFailoverController:doFence
org.apache.hadoop.ha.FailoverController:preFailoverChecks
org.apache.hadoop.ha.HAAdmin:transitionToActive
org.apache.hadoop.ha.HealthMonitor:createProxy
org.apache.hadoop.ha.ZKFailoverController:doCedeActive
org.apache.hadoop.ha.HAAdmin:getServiceState
org.apache.hadoop.ha.ZKFailoverController:becomeActive
org.apache.hadoop.ha.ZKFailoverController:becomeStandby
org.apache.hadoop.ha.HAAdmin:transitionToStandby
org.apache.hadoop.ha.FailoverController:tryGracefulFence
org.apache.hadoop.ha.HAAdmin:checkHealth
org.apache.hadoop.ha.HAServiceTarget:getProxy
org.apache.hadoop.io.SequenceFile$Sorter:sort
org.apache.hadoop.io.SequenceFile$Sorter:sortAndIterate
org.apache.hadoop.io.SequenceFile$Sorter:mergePass
org.apache.hadoop.io.SequenceFile$Sorter:merge
org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:merge
org.apache.hadoop.io.SequenceFile$Sorter:sortPass
org.apache.hadoop.io.SequenceFile$Sorter$SortPass:run
org.apache.hadoop.io.SequenceFile$Sorter:cloneFileAttributes
org.apache.hadoop.io.SequenceFile$Sorter$SortPass:flush
org.apache.hadoop.fs.FileUtil:createJarWithClassPath
org.apache.hadoop.io.SequenceFile:createWriter
org.apache.hadoop.fs.FileContext:getLocalFSFileContext
org.apache.hadoop.ha.ZKFailoverController$3:run
org.apache.hadoop.ha.ZKFailoverController:access$400
org.apache.hadoop.ha.ZKFailoverController:doGracefulFailover
org.apache.hadoop.ha.HAAdmin:gracefulFailoverThroughZKFCs
org.apache.hadoop.ha.HAServiceTarget:getZKFCProxy
org.apache.hadoop.ha.protocolPB.ZKFCProtocolClientSideTranslatorPB:<init>
org.apache.hadoop.ipc.RPC:getProxy
org.apache.hadoop.fs.FileSystem$Cache:getUnique
org.apache.hadoop.fs.FileSystem:addFileSystemForTesting
org.apache.hadoop.fs.FileSystem$Cache:get
org.apache.hadoop.fs.FsShell:main
org.apache.hadoop.fs.s3.MigrationTool:main
org.apache.hadoop.util.ToolRunner:run
org.apache.hadoop.util.GenericOptionsParser:<init>
org.apache.hadoop.util.GenericOptionsParser:parseGeneralOptions
org.apache.hadoop.security.UserGroupInformation:getBestUGI
org.apache.hadoop.fs.viewfs.ViewFileSystem:<init>
org.apache.hadoop.fs.viewfs.InodeTree:<init>
org.apache.hadoop.security.SaslRpcServer:<init>
org.apache.hadoop.tools.GetGroupsBase:run
org.apache.hadoop.tools.GetGroupsBase:getUgmProtocol
org.apache.hadoop.security.UserGroupInformation:loginUserFromKeytabAndReturnUGI
org.apache.hadoop.ipc.RPC:waitForProtocolProxy
org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:<init>
org.apache.hadoop.fs.FileContext:getFileContext
org.apache.hadoop.ipc.RPC:getProtocolProxy
org.apache.hadoop.fs.FileContext:<init>
org.apache.hadoop.fs.FileSystem$Cache$Key:<init>
org.apache.hadoop.security.SecurityUtil:doAsCurrentUser
org.apache.hadoop.util.GenericOptionsParser:processGeneralOptions
org.apache.hadoop.security.UserGroupInformation:main
org.apache.hadoop.security.SaslRpcServer:create
org.apache.hadoop.security.SecurityUtil:doAsLoginUser
org.apache.hadoop.ha.ZKFailoverController:gracefulFailoverToYou
org.apache.hadoop.security.SecurityUtil:doAsLoginUserOrFatal
org.apache.hadoop.ipc.Client$Connection$1:run
org.apache.hadoop.security.UserGroupInformation:isLoginKeytabBased
org.apache.hadoop.ha.ZKFailoverController:cedeActive
org.apache.hadoop.ipc.Client$Connection:shouldAuthenticateOverKrb
org.apache.hadoop.security.UserGroupInformation:getCurrentUser
org.apache.hadoop.security.UserGroupInformation:getLoginUser
org.apache.hadoop.io.file.tfile.TFile:main
org.apache.hadoop.io.SequenceFile$RecordCompressWriter:<init>
org.apache.hadoop.io.SequenceFile$BlockCompressWriter:<init>
org.apache.hadoop.fs.FsShell:run
org.apache.hadoop.fs.shell.Command:runAll
org.apache.hadoop.fs.shell.Command:run
org.apache.hadoop.fs.shell.Command:processRawArguments
org.apache.hadoop.fs.shell.Command:expandArguments
org.apache.hadoop.fs.shell.Command:expandArgument
org.apache.hadoop.fs.shell.CommandWithDestination:getRemoteDestination
org.apache.hadoop.fs.shell.Display$Text:getInputStream
org.apache.hadoop.io.ArrayFile$Reader:<init>
org.apache.hadoop.io.SetFile$Reader:<init>
org.apache.hadoop.io.MapFile$Reader:<init>
org.apache.hadoop.io.MapFile$Reader:open
org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:next
org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:adjustPriorityQueue
org.apache.hadoop.fs.shell.Display$TextRecordInputStream:<init>
org.apache.hadoop.io.MapFile$Reader:createDataFileReader
org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:nextRawKey
org.apache.hadoop.io.MapFile$Writer:<init>
org.apache.hadoop.io.BloomMapFile$Reader:initBloomFilter
org.apache.hadoop.security.Credentials:readTokenStorageFile
org.apache.hadoop.io.file.tfile.TFileDumper:dumpInfo
org.apache.hadoop.security.Credentials:writeTokenStorageFile
org.apache.hadoop.io.SequenceFile$Writer:<init>
org.apache.hadoop.fs.shell.PathData:expandAsGlob
org.apache.hadoop.util.GenericOptionsParser:validateFiles
org.apache.hadoop.util.GenericOptionsParser:getLibJars
org.apache.hadoop.io.SequenceFile$Reader:<init>
org.apache.hadoop.fs.viewfs.ViewFileSystem$1:getTargetFileSystem
org.apache.hadoop.fs.shell.Delete$Rm:processPath
org.apache.hadoop.fs.shell.Delete$Rm:moveToTrash
org.apache.hadoop.fs.FsShell:getCurrentTrashDir
org.apache.hadoop.fs.FsShell:getTrash
org.apache.hadoop.fs.shell.Delete$Expunge:processArguments
org.apache.hadoop.fs.ChecksumFileSystem:copyToLocalFile
org.apache.hadoop.fs.FilterFileSystem:copyToLocalFile
org.apache.hadoop.fs.FileSystem:moveToLocalFile
org.apache.hadoop.fs.LocalDirAllocator:getLocalPathToRead
org.apache.hadoop.fs.LocalDirAllocator:createTmpFileForWrite
org.apache.hadoop.fs.LocalDirAllocator:getLocalPathForWrite
org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:createTmpFileForWrite
org.apache.hadoop.fs.LocalDirAllocator:getAllLocalPathsToRead
org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:getLocalPathToRead
org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:getLocalPathForWrite
org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:getAllLocalPathsToRead
org.apache.hadoop.fs.shell.CommandWithDestination:recursePath
org.apache.hadoop.fs.shell.CommandWithDestination:processPathArgument
org.apache.hadoop.fs.shell.CommandWithDestination:processPath
org.apache.hadoop.fs.shell.CommandWithDestination:getTargetPath
org.apache.hadoop.fs.shell.Command:processArguments
org.apache.hadoop.fs.shell.Command:processArgument
org.apache.hadoop.fs.shell.Command:processPathArgument
org.apache.hadoop.fs.shell.Command:processPaths
org.apache.hadoop.fs.shell.Command:recursePath
org.apache.hadoop.fs.shell.CommandWithDestination:copyFileToTarget
org.apache.hadoop.fs.shell.CommandWithDestination:copyStreamToTarget
org.apache.hadoop.fs.shell.CopyCommands$AppendToFile:expandArgument
org.apache.hadoop.fs.shell.Tail:expandArgument
org.apache.hadoop.fs.shell.CommandWithDestination:getLocalDestination
org.apache.hadoop.fs.shell.PathData:getPathDataForChild
org.apache.hadoop.fs.shell.CopyCommands$Merge:processOptions
org.apache.hadoop.fs.shell.PathData:getDirectoryContents
org.apache.hadoop.fs.shell.PathData:suffix
org.apache.hadoop.fs.shell.CopyCommands$Put:expandArgument
org.apache.hadoop.fs.FilterFileSystem:completeLocalOutput
org.apache.hadoop.fs.FileSystem:completeLocalOutput
org.apache.hadoop.fs.FileSystem:moveFromLocalFile
org.apache.hadoop.fs.FilterFileSystem:copyFromLocalFile
org.apache.hadoop.fs.FileSystem:copyToLocalFile
org.apache.hadoop.conf.Configuration:getLocalPath
org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:confChanged
org.apache.hadoop.fs.shell.PathData:<init>
org.apache.hadoop.io.SecureIOUtils:<clinit>
org.apache.hadoop.fs.FileSystem:copyFromLocalFile
org.apache.hadoop.fs.FileSystemLinkResolver:resolve
org.apache.hadoop.fs.FsUrlConnection:getInputStream
org.apache.hadoop.fs.HarFileSystem:initialize
org.apache.hadoop.fs.Path:getFileSystem
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:<init>
org.apache.hadoop.fs.Trash:moveToAppropriateTrash
org.apache.hadoop.fs.Trash:<init>
org.apache.hadoop.fs.FileSystem:getLocal
org.apache.hadoop.fs.FileSystem:getNamed
org.apache.hadoop.fs.FileSystem$1:run
org.apache.hadoop.fs.FileSystem:getFSofPath
org.apache.hadoop.fs.FsUrlConnection:connect
org.apache.hadoop.fs.FsShell:getFS
org.apache.hadoop.fs.FileContext:createSymlink
org.apache.hadoop.ipc.WritableRpcEngine:getProxy
org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:invoke
org.apache.hadoop.ipc.WritableRpcEngine$Invoker:invoke
org.apache.hadoop.ipc.ProtobufRpcEngine:getProxy
org.apache.hadoop.ipc.ProtobufRpcEngine:getProtocolMetaInfoProxy
org.apache.hadoop.ipc.WritableRpcEngine$Invoker:<init>
org.apache.hadoop.ipc.Client:call
org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:<init>
org.apache.hadoop.http.HttpServer:<init>
org.apache.hadoop.ipc.ProtobufRpcEngine:getClient
org.apache.hadoop.ipc.WritableRpcEngine:getClient
org.apache.hadoop.ipc.ClientCache:getClient
org.apache.hadoop.io.compress.DefaultCodec:getCompressorType
org.apache.hadoop.io.compress.DefaultCodec:getDecompressorType
org.apache.hadoop.io.file.tfile.Compression$Algorithm$2:createCompressionStream
org.apache.hadoop.io.compress.DefaultCodec:createOutputStream
org.apache.hadoop.io.compress.DefaultCodec:createCompressor
org.apache.hadoop.io.compress.GzipCodec:createOutputStream
org.apache.hadoop.io.file.tfile.Compression$Algorithm$2:createDecompressionStream
org.apache.hadoop.io.compress.DefaultCodec:createInputStream
org.apache.hadoop.io.compress.DefaultCodec:createDecompressor
org.apache.hadoop.io.compress.GzipCodec:createInputStream
org.apache.hadoop.io.compress.zlib.ZlibFactory:getZlibCompressorType
org.apache.hadoop.io.compress.zlib.ZlibFactory:getZlibDecompressorType
org.apache.hadoop.io.compress.GzipCodec:getCompressorType
org.apache.hadoop.io.compress.zlib.ZlibFactory:getZlibCompressor
org.apache.hadoop.io.compress.GzipCodec:getDecompressorType
org.apache.hadoop.io.compress.GzipCodec:createCompressor
org.apache.hadoop.io.compress.zlib.ZlibFactory:getZlibDecompressor
org.apache.hadoop.io.compress.GzipCodec:createDecompressor
org.apache.hadoop.jmx.JMXJsonServlet:doGet
org.apache.hadoop.conf.ConfServlet:doGet
org.apache.hadoop.metrics.MetricsServlet:doGet
org.apache.hadoop.http.HttpServer$StackServlet:doGet
org.apache.hadoop.http.HttpServer:isInstrumentationAccessAllowed
org.apache.hadoop.log.LogLevel$Servlet:doGet
org.apache.hadoop.http.AdminAuthorizedServlet:doGet
org.apache.hadoop.fs.local.LocalFs:<init>
org.apache.hadoop.fs.local.RawLocalFs:<init>
org.apache.hadoop.fs.ftp.FtpFs:<init>
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:initialize
org.apache.hadoop.fs.FileSystem:access$200
org.apache.hadoop.fs.s3native.NativeS3FileSystem:initialize
org.apache.hadoop.fs.DelegateToFileSystem:<init>
org.apache.hadoop.fs.FilterFileSystem:initialize
org.apache.hadoop.fs.FileSystem:createFileSystem
org.apache.hadoop.fs.LocalFileSystem:initialize
org.apache.hadoop.fs.RawLocalFileSystem:initialize
org.apache.hadoop.fs.ftp.FTPFileSystem:initialize
org.apache.hadoop.fs.s3.S3FileSystem:initialize
org.apache.hadoop.ipc.Server:<init>
org.apache.hadoop.security.SecurityUtil:<clinit>
org.apache.hadoop.security.LdapGroupsMapping:setConf
org.apache.hadoop.io.retry.RetryUtils:getMultipleLinearRandomRetry
org.apache.hadoop.io.compress.bzip2.Bzip2Factory:isNativeBzip2Loaded
org.apache.hadoop.io.nativeio.NativeIO$POSIX:<clinit>
org.apache.hadoop.io.SequenceFile$Reader:handleChecksumException
org.apache.hadoop.http.HttpConfig:<clinit>
org.apache.hadoop.ha.ZKFCRpcServer:<init>
org.apache.hadoop.fs.FileSystem:get
org.apache.hadoop.fs.FileSystem:isSymlinksEnabled
org.apache.hadoop.ipc.Client$ConnectionId:getConnectionId
org.apache.hadoop.ipc.Client:getTimeout
org.apache.hadoop.fs.FileSystem$Cache:getInternal
org.apache.hadoop.security.ssl.SSLFactory:<init>
org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:init
org.apache.hadoop.ipc.Client:<init>
org.apache.hadoop.io.compress.zlib.ZlibFactory:isNativeZlibLoaded
org.apache.hadoop.http.HttpServer:addDefaultApps
org.apache.hadoop.http.HttpServer:hasAdministratorAccess
org.apache.hadoop.util.NativeCodeLoader:getLoadNativeLibraries
org.apache.hadoop.fs.FileSystem:initialize
