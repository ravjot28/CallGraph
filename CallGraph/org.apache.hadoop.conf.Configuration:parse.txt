org.apache.hadoop.conf.Configuration:updateConnectAddr
org.apache.hadoop.fs.FsShell:main
org.apache.hadoop.fs.s3.MigrationTool:main
org.apache.hadoop.util.ToolRunner:run
org.apache.hadoop.util.GenericOptionsParser:<init>
org.apache.hadoop.util.GenericOptionsParser:parseGeneralOptions
org.apache.hadoop.security.SecurityUtil:<clinit>
org.apache.hadoop.http.HttpServer:<init>
org.apache.hadoop.security.ssl.SSLFactory:<init>
org.apache.hadoop.util.GenericOptionsParser:processGeneralOptions
org.apache.hadoop.util.NativeCodeLoader:setLoadNativeLibraries
org.apache.hadoop.security.ssl.SSLFactory:readSSLConfiguration
org.apache.hadoop.conf.Configuration:setBooleanIfUnset
org.apache.hadoop.ha.ZKFailoverController$1:run
org.apache.hadoop.ha.ZKFailoverController:access$000
org.apache.hadoop.ha.ZKFailoverController:doRun
org.apache.hadoop.ha.ZKFailoverController:initRPC
org.apache.hadoop.ha.HAAdmin:run
org.apache.hadoop.ha.HAAdmin:runCmd
org.apache.hadoop.ha.HAAdmin:failover
org.apache.hadoop.ha.FailoverController:failover
org.apache.hadoop.ha.HealthMonitor$MonitorDaemon:run
org.apache.hadoop.ha.HealthMonitor:access$500
org.apache.hadoop.ha.HealthMonitor:loopUntilConnected
org.apache.hadoop.ha.HealthMonitor:tryConnect
org.apache.hadoop.ha.ZKFailoverController$2:run
org.apache.hadoop.ha.ZKFailoverController:access$300
org.apache.hadoop.ha.ZKFailoverController$ElectorCallbacks:becomeActive
org.apache.hadoop.ha.ZKFailoverController:access$900
org.apache.hadoop.ha.ZKFailoverController$ElectorCallbacks:becomeStandby
org.apache.hadoop.ha.ZKFailoverController:access$1000
org.apache.hadoop.ha.ZKFailoverController$ElectorCallbacks:fenceOldActive
org.apache.hadoop.ha.ZKFailoverController:access$1100
org.apache.hadoop.ha.ZKFailoverController:fenceOldActive
org.apache.hadoop.ha.ZKFailoverController:doFence
org.apache.hadoop.ha.FailoverController:preFailoverChecks
org.apache.hadoop.ha.HAAdmin:transitionToActive
org.apache.hadoop.ha.HealthMonitor:createProxy
org.apache.hadoop.ha.ZKFailoverController:doCedeActive
org.apache.hadoop.ha.HAAdmin:getServiceState
org.apache.hadoop.ha.ZKFailoverController:becomeActive
org.apache.hadoop.ha.ZKFailoverController:becomeStandby
org.apache.hadoop.ha.HAAdmin:transitionToStandby
org.apache.hadoop.ha.FailoverController:tryGracefulFence
org.apache.hadoop.ha.HAAdmin:checkHealth
org.apache.hadoop.ha.HAServiceTarget:getProxy
org.apache.hadoop.ipc.WritableRpcEngine:getServer
org.apache.hadoop.ipc.ProtobufRpcEngine:getServer
org.apache.hadoop.ipc.WritableRpcEngine$Server:<init>
org.apache.hadoop.ipc.ProtobufRpcEngine$Server:<init>
org.apache.hadoop.ipc.RPC$Server:<init>
org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolClientSideTranslatorPB:isMethodSupported
org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolClientSideTranslatorPB:isMethodSupported
org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolClientSideTranslatorPB:isMethodSupported
org.apache.hadoop.ha.ZKFailoverController$3:run
org.apache.hadoop.ha.ZKFailoverController:access$400
org.apache.hadoop.ha.ZKFailoverController:doGracefulFailover
org.apache.hadoop.ha.HAAdmin:gracefulFailoverThroughZKFCs
org.apache.hadoop.ha.HAServiceTarget:getZKFCProxy
org.apache.hadoop.ha.ZKFCRpcServer:<init>
org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:<init>
org.apache.hadoop.ipc.RPC$Server:initProtocolMetaInfo
org.apache.hadoop.ipc.RpcClientUtil:isMethodSupported
org.apache.hadoop.ha.protocolPB.ZKFCProtocolClientSideTranslatorPB:<init>
org.apache.hadoop.ipc.RPC:setProtocolEngine
org.apache.hadoop.io.compress.zlib.ZlibFactory:setCompressionLevel
org.apache.hadoop.io.compress.zlib.ZlibFactory:setCompressionStrategy
org.apache.hadoop.fs.shell.Delete$Rm:processPath
org.apache.hadoop.fs.shell.Delete$Rm:moveToTrash
org.apache.hadoop.fs.Trash:moveToAppropriateTrash
org.apache.hadoop.io.file.tfile.Compression$Algorithm$1:createDecompressionStream
org.apache.hadoop.ipc.Client:setPingInterval
org.apache.hadoop.ipc.Client:setConnectTimeout
org.apache.hadoop.ha.FailoverController:<init>
org.apache.hadoop.io.compress.bzip2.Bzip2Factory:setBlockSize
org.apache.hadoop.io.file.tfile.Compression$Algorithm$2:createDecompressionStream
org.apache.hadoop.io.file.tfile.Compression$Algorithm$1:createCompressionStream
org.apache.hadoop.io.compress.bzip2.Bzip2Factory:setWorkFactor
org.apache.hadoop.io.MapFile$Writer:setIndexInterval
org.apache.hadoop.fs.ftp.FTPFileSystem:initialize
org.apache.hadoop.io.file.tfile.Compression$Algorithm$2:createCompressionStream
org.apache.hadoop.conf.Configuration:setTimeDuration
org.apache.hadoop.io.DefaultStringifier:store
org.apache.hadoop.conf.Configuration:setSocketAddr
org.apache.hadoop.conf.Configuration:setBoolean
org.apache.hadoop.io.DefaultStringifier:storeArray
org.apache.hadoop.fs.viewfs.ConfigUtil:setHomeDirConf
org.apache.hadoop.conf.Configuration:setFloat
org.apache.hadoop.conf.Configuration:setIfUnset
org.apache.hadoop.conf.Configuration:setClass
org.apache.hadoop.fs.viewfs.ConfigUtil:addLink
org.apache.hadoop.fs.permission.FsPermission:setUMask
org.apache.hadoop.conf.Configuration:setDouble
org.apache.hadoop.conf.Configuration:setEnum
org.apache.hadoop.conf.Configuration:setLong
org.apache.hadoop.conf.ReconfigurableBase:reconfigureProperty
org.apache.hadoop.conf.Configuration:setInt
org.apache.hadoop.conf.Configuration:setStrings
org.apache.hadoop.conf.Configuration:setPattern
org.apache.hadoop.conf.Configuration:readFields
org.apache.hadoop.security.SecurityUtil:setAuthenticationMethod
org.apache.hadoop.io.SequenceFile:setDefaultCompressionType
org.apache.hadoop.io.compress.CompressionCodecFactory:setCodecClasses
org.apache.hadoop.fs.FileSystem:setDefaultUri
org.apache.hadoop.fs.viewfs.ViewFs:<init>
org.apache.hadoop.fs.viewfs.ViewFileSystem:<init>
org.apache.hadoop.fs.viewfs.ViewFileSystem:initialize
org.apache.hadoop.fs.viewfs.ViewFs$1:<init>
org.apache.hadoop.fs.viewfs.ViewFileSystem$1:<init>
org.apache.hadoop.conf.ReconfigurationServlet:doGet
org.apache.hadoop.conf.ReconfigurationServlet:printConf
org.apache.hadoop.ha.ShellCommandFencer:tryFence
org.apache.hadoop.fs.viewfs.InodeTree:<init>
org.apache.hadoop.conf.ReconfigurationUtil:getChangedProperties
org.apache.hadoop.security.AuthenticationFilterInitializer:initFilter
org.apache.hadoop.ha.ShellCommandFencer:setConfAsEnvVars
org.apache.hadoop.io.retry.RetryUtils:getDefaultRetryPolicy
org.apache.hadoop.io.SetFile$Writer:<init>
org.apache.hadoop.io.MapFile:main
org.apache.hadoop.io.BloomMapFile$Writer:<init>
org.apache.hadoop.io.ArrayFile$Writer:<init>
org.apache.hadoop.io.SequenceFile$Sorter:sort
org.apache.hadoop.io.SequenceFile$Sorter:sortAndIterate
org.apache.hadoop.io.SequenceFile$Sorter:mergePass
org.apache.hadoop.io.SequenceFile$Sorter:merge
org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:merge
org.apache.hadoop.io.SequenceFile$Sorter:sortPass
org.apache.hadoop.io.SequenceFile$Sorter$SortPass:run
org.apache.hadoop.io.MapFile$Writer:<init>
org.apache.hadoop.io.SequenceFile$Sorter:cloneFileAttributes
org.apache.hadoop.io.SequenceFile$Sorter$SortPass:flush
org.apache.hadoop.io.MapFile:fix
org.apache.hadoop.io.SequenceFile:createWriter
org.apache.hadoop.util.NativeLibraryChecker:main
org.apache.hadoop.io.compress.BZip2Codec:createCompressor
org.apache.hadoop.io.compress.BZip2Codec:getDecompressorType
org.apache.hadoop.io.compress.BZip2Codec:createInputStream
org.apache.hadoop.io.compress.BZip2Codec:createDecompressor
org.apache.hadoop.io.compress.BZip2Codec:getCompressorType
org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getLibraryName
org.apache.hadoop.io.compress.BZip2Codec:createOutputStream
org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getBzip2Compressor
org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getBzip2DecompressorType
org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getBzip2Decompressor
org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getBzip2CompressorType
org.apache.hadoop.ha.ZKFailoverController:formatZK
org.apache.hadoop.ha.ZKFailoverController:confirmFormat
org.apache.hadoop.net.TableMapping:reloadCachedMappings
org.apache.hadoop.net.TableMapping$RawTableMapping:resolve
org.apache.hadoop.net.TableMapping$RawTableMapping:reloadCachedMappings
org.apache.hadoop.io.file.tfile.Compression$Algorithm$1:getCodec
org.apache.hadoop.fs.LocalDirAllocator:getLocalPathToRead
org.apache.hadoop.fs.LocalDirAllocator:createTmpFileForWrite
org.apache.hadoop.fs.LocalDirAllocator:getLocalPathForWrite
org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:createTmpFileForWrite
org.apache.hadoop.fs.LocalDirAllocator:getAllLocalPathsToRead
org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:getLocalPathToRead
org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:getLocalPathForWrite
org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:getAllLocalPathsToRead
org.apache.hadoop.http.lib.StaticUserWebFilter:initFilter
org.apache.hadoop.fs.s3.Jets3tFileSystemStore:retrieveBlock
org.apache.hadoop.ipc.RPC:waitForProxy
org.apache.hadoop.tools.GetGroupsBase:run
org.apache.hadoop.tools.GetGroupsBase:getUgmProtocol
org.apache.hadoop.ipc.RPC:getProxy
org.apache.hadoop.ipc.WritableRpcEngine:getProxy
org.apache.hadoop.ipc.ProtobufRpcEngine:getProxy
org.apache.hadoop.ipc.ProtobufRpcEngine:getProtocolMetaInfoProxy
org.apache.hadoop.ipc.ProtobufRpcEngine:getClient
org.apache.hadoop.ipc.WritableRpcEngine$Invoker:<init>
org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:<init>
org.apache.hadoop.ipc.WritableRpcEngine:getClient
org.apache.hadoop.ipc.ClientCache:getClient
org.apache.hadoop.ipc.RpcClientUtil:getProtocolMetaInfoProxy
org.apache.hadoop.ipc.RPC:waitForProtocolProxy
org.apache.hadoop.ipc.RPC:getProtocolProxy
org.apache.hadoop.ipc.Client:<init>
org.apache.hadoop.net.NetUtils:getSocketFactory
org.apache.hadoop.ha.ZKFCRpcServer:gracefulFailover
org.apache.hadoop.ha.ZKFailoverController:run
org.apache.hadoop.ha.ZKFCRpcServer:cedeActive
org.apache.hadoop.ipc.Client$Connection:access$1500
org.apache.hadoop.io.BloomMapFile$Reader:<init>
org.apache.hadoop.io.file.tfile.TFile:main
org.apache.hadoop.io.SequenceFile$RecordCompressWriter:<init>
org.apache.hadoop.io.SequenceFile$BlockCompressWriter:<init>
org.apache.hadoop.fs.FsShell:run
org.apache.hadoop.fs.shell.Command:runAll
org.apache.hadoop.fs.shell.Command:run
org.apache.hadoop.fs.shell.Command:processRawArguments
org.apache.hadoop.fs.shell.Command:expandArguments
org.apache.hadoop.fs.shell.Command:expandArgument
org.apache.hadoop.fs.shell.CommandWithDestination:getRemoteDestination
org.apache.hadoop.fs.shell.Display$Text:getInputStream
org.apache.hadoop.io.ArrayFile$Reader:<init>
org.apache.hadoop.io.SetFile$Reader:<init>
org.apache.hadoop.io.MapFile$Reader:<init>
org.apache.hadoop.io.MapFile$Reader:open
org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:next
org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:adjustPriorityQueue
org.apache.hadoop.fs.shell.Display$TextRecordInputStream:<init>
org.apache.hadoop.io.MapFile$Reader:createDataFileReader
org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:nextRawKey
org.apache.hadoop.io.BloomMapFile$Reader:initBloomFilter
org.apache.hadoop.security.Credentials:readTokenStorageFile
org.apache.hadoop.io.file.tfile.TFileDumper:dumpInfo
org.apache.hadoop.security.Credentials:writeTokenStorageFile
org.apache.hadoop.io.SequenceFile$Writer:<init>
org.apache.hadoop.fs.shell.PathData:expandAsGlob
org.apache.hadoop.util.GenericOptionsParser:validateFiles
org.apache.hadoop.util.GenericOptionsParser:getLibJars
org.apache.hadoop.io.SequenceFile$Reader:<init>
org.apache.hadoop.fs.viewfs.ViewFileSystem$1:getTargetFileSystem
org.apache.hadoop.fs.FsShell:getCurrentTrashDir
org.apache.hadoop.fs.FsShell:getTrash
org.apache.hadoop.fs.shell.Delete$Expunge:processArguments
org.apache.hadoop.fs.ChecksumFileSystem:copyToLocalFile
org.apache.hadoop.fs.FilterFileSystem:copyToLocalFile
org.apache.hadoop.fs.FileSystem:moveToLocalFile
org.apache.hadoop.fs.shell.CommandWithDestination:recursePath
org.apache.hadoop.fs.shell.CommandWithDestination:processPathArgument
org.apache.hadoop.fs.shell.CommandWithDestination:processPath
org.apache.hadoop.fs.shell.CommandWithDestination:getTargetPath
org.apache.hadoop.fs.shell.Command:processArguments
org.apache.hadoop.fs.shell.Command:processArgument
org.apache.hadoop.fs.shell.Command:processPathArgument
org.apache.hadoop.fs.shell.Command:processPaths
org.apache.hadoop.fs.shell.Command:recursePath
org.apache.hadoop.fs.shell.CommandWithDestination:copyFileToTarget
org.apache.hadoop.fs.shell.CommandWithDestination:copyStreamToTarget
org.apache.hadoop.fs.shell.CopyCommands$AppendToFile:expandArgument
org.apache.hadoop.fs.shell.Tail:expandArgument
org.apache.hadoop.fs.shell.CommandWithDestination:getLocalDestination
org.apache.hadoop.fs.shell.PathData:getPathDataForChild
org.apache.hadoop.fs.shell.CopyCommands$Merge:processOptions
org.apache.hadoop.fs.shell.PathData:getDirectoryContents
org.apache.hadoop.fs.shell.PathData:suffix
org.apache.hadoop.fs.shell.CopyCommands$Put:expandArgument
org.apache.hadoop.fs.FilterFileSystem:completeLocalOutput
org.apache.hadoop.fs.FileSystem:completeLocalOutput
org.apache.hadoop.fs.FileSystem:moveFromLocalFile
org.apache.hadoop.fs.FilterFileSystem:copyFromLocalFile
org.apache.hadoop.fs.FileSystem:copyToLocalFile
org.apache.hadoop.conf.Configuration:getLocalPath
org.apache.hadoop.fs.shell.PathData:<init>
org.apache.hadoop.io.SecureIOUtils:<clinit>
org.apache.hadoop.fs.FileSystem:copyFromLocalFile
org.apache.hadoop.fs.FileSystemLinkResolver:resolve
org.apache.hadoop.fs.FsUrlConnection:getInputStream
org.apache.hadoop.fs.HarFileSystem:initialize
org.apache.hadoop.fs.Path:getFileSystem
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:<init>
org.apache.hadoop.fs.Trash:<init>
org.apache.hadoop.fs.FileSystem:getLocal
org.apache.hadoop.fs.FileSystem:getNamed
org.apache.hadoop.fs.FileSystem$1:run
org.apache.hadoop.fs.FileSystem:getFSofPath
org.apache.hadoop.fs.FsUrlConnection:connect
org.apache.hadoop.fs.FsShell:getFS
org.apache.hadoop.fs.FileSystem:newInstanceLocal
org.apache.hadoop.fs.FileSystem$2:run
org.apache.hadoop.security.UserGroupInformation:getUGIFromTicketCache
org.apache.hadoop.fs.FileSystem:get
org.apache.hadoop.fs.FileSystem:newInstance
org.apache.hadoop.ipc.Server$Listener$Reader:run
org.apache.hadoop.ipc.Server$Listener$Reader:doRunLoop
org.apache.hadoop.ipc.Server$Listener:doRead
org.apache.hadoop.ipc.Server$Connection:readAndProcess
org.apache.hadoop.ipc.Server$Connection:unwrapPacketAndProcessRpcs
org.apache.hadoop.ipc.Server$Connection:processOneRpc
org.apache.hadoop.ipc.Server$Connection:processRpcOutOfBandRequest
org.apache.hadoop.ipc.Server$Connection:saslReadAndProcess
org.apache.hadoop.ipc.Server$Connection:saslProcess
org.apache.hadoop.ipc.Server$Connection:processSaslMessage
org.apache.hadoop.ipc.Server$Connection:buildSaslNegotiateResponse
org.apache.hadoop.ipc.Server:<init>
org.apache.hadoop.ipc.Server$Connection:createSaslServer
org.apache.hadoop.ipc.Server:buildNegotiateResponse
org.apache.hadoop.fs.FileUtil:createJarWithClassPath
org.apache.hadoop.fs.FileContext:getLocalFSFileContext
org.apache.hadoop.fs.FileSystem$Cache:getUnique
org.apache.hadoop.fs.FileSystem:addFileSystemForTesting
org.apache.hadoop.fs.FileSystem$Cache:get
org.apache.hadoop.security.UserGroupInformation:getBestUGI
org.apache.hadoop.security.SaslRpcServer:<init>
org.apache.hadoop.security.UserGroupInformation:loginUserFromKeytabAndReturnUGI
org.apache.hadoop.fs.FileContext:getFileContext
org.apache.hadoop.fs.FileContext:<init>
org.apache.hadoop.fs.FileSystem$Cache$Key:<init>
org.apache.hadoop.security.SecurityUtil:doAsCurrentUser
org.apache.hadoop.security.UserGroupInformation:main
org.apache.hadoop.security.SaslRpcServer:create
org.apache.hadoop.security.SecurityUtil:doAsLoginUser
org.apache.hadoop.ha.ZKFailoverController:gracefulFailoverToYou
org.apache.hadoop.security.SecurityUtil:doAsLoginUserOrFatal
org.apache.hadoop.ipc.Client$Connection$1:run
org.apache.hadoop.security.UserGroupInformation:isLoginKeytabBased
org.apache.hadoop.ha.ZKFailoverController:cedeActive
org.apache.hadoop.ipc.Client$Connection:shouldAuthenticateOverKrb
org.apache.hadoop.security.UserGroupInformation:getCurrentUser
org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule:commit
org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:invoke
org.apache.hadoop.ipc.WritableRpcEngine$Invoker:invoke
org.apache.hadoop.ipc.Client:call
org.apache.hadoop.ipc.Client:getConnection
org.apache.hadoop.ipc.Client$Connection:access$2600
org.apache.hadoop.security.SecurityUtil:login
org.apache.hadoop.security.UserGroupInformation:checkTGTAndReloginFromKeytab
org.apache.hadoop.security.UserGroupInformation$1:run
org.apache.hadoop.http.HttpServer:addDefaultServlets
org.apache.hadoop.http.HttpServer:addServlet
org.apache.hadoop.ipc.Client$Connection:setupIOstreams
org.apache.hadoop.security.UserGroupInformation:loginUserFromKeytab
org.apache.hadoop.io.SecureIOUtils:openFSDataInputStream
org.apache.hadoop.io.SecureIOUtils:openForRead
org.apache.hadoop.security.UserGroupInformation:reloginFromKeytab
org.apache.hadoop.security.UserGroupInformation:reloginFromTicketCache
org.apache.hadoop.security.UserGroupInformation:spawnAutoRenewalThreadForUserCreds
org.apache.hadoop.ipc.Client$Connection:<init>
org.apache.hadoop.security.SecurityUtil:openSecureHttpConnection
org.apache.hadoop.io.SecureIOUtils:openForRandomRead
org.apache.hadoop.http.HttpServer:addInternalServlet
org.apache.hadoop.security.UserGroupInformation:access$100
org.apache.hadoop.security.UserGroupInformation:isSecurityEnabled
org.apache.hadoop.io.SecureIOUtils:forceSecureOpenFSDataInputStream
org.apache.hadoop.io.SecureIOUtils:forceSecureOpenForRandomRead
org.apache.hadoop.io.SecureIOUtils:forceSecureOpenForRead
org.apache.hadoop.ipc.Server$Connection:processConnectionContext
org.apache.hadoop.ipc.Server$Connection:authorizeConnection
org.apache.hadoop.ipc.Server:access$3500
org.apache.hadoop.ipc.Server:authorize
org.apache.hadoop.jmx.JMXJsonServlet:doGet
org.apache.hadoop.conf.ConfServlet:doGet
org.apache.hadoop.metrics.MetricsServlet:doGet
org.apache.hadoop.http.HttpServer$StackServlet:doGet
org.apache.hadoop.http.HttpServer:isInstrumentationAccessAllowed
org.apache.hadoop.log.LogLevel$Servlet:doGet
org.apache.hadoop.http.AdminAuthorizedServlet:doGet
org.apache.hadoop.http.HttpServer:hasAdministratorAccess
org.apache.hadoop.security.authorize.ServiceAuthorizationManager:authorize
org.apache.hadoop.http.HttpServer:userHasAdministratorAccess
org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getLinkTarget
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getFileStatus
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:listStatus
org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getFileStatus
org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:listStatus
org.apache.hadoop.io.SecureIOUtils:checkStat
org.apache.hadoop.security.authorize.ProxyUsers:authorize
org.apache.hadoop.security.UserGroupInformation:print
org.apache.hadoop.security.authorize.AccessControlList:isUserAllowed
org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getFileLinkStatus
org.apache.hadoop.security.UserGroupInformation:createProxyUserForTesting
org.apache.hadoop.security.UserGroupInformation:getLoginUser
org.apache.hadoop.security.UserGroupInformation:isAuthenticationMethodEnabled
org.apache.hadoop.security.UserGroupInformation:getGroupNames
org.apache.hadoop.security.UserGroupInformation:createUserForTesting
org.apache.hadoop.security.UserGroupInformation:setConfiguration
org.apache.hadoop.security.UserGroupInformation:ensureInitialized
org.apache.hadoop.security.HadoopKerberosName:main
org.apache.hadoop.security.UserGroupInformation:initialize
org.apache.hadoop.io.compress.CompressionCodecFactory:main
org.apache.hadoop.io.compress.CompressionCodecFactory:<init>
org.apache.hadoop.fs.ftp.FTPFileSystem:delete
org.apache.hadoop.fs.ftp.FTPFileSystem:getWorkingDirectory
org.apache.hadoop.fs.ftp.FTPFileSystem:isFile
org.apache.hadoop.fs.ftp.FTPFileSystem:exists
org.apache.hadoop.fs.ftp.FTPFileSystem:open
org.apache.hadoop.fs.ftp.FTPFileSystem:create
org.apache.hadoop.fs.ftp.FTPFileSystem:mkdirs
org.apache.hadoop.fs.ftp.FTPFileSystem:listStatus
org.apache.hadoop.fs.ftp.FTPFileSystem:rename
org.apache.hadoop.fs.ftp.FTPFileSystem:getHomeDirectory
org.apache.hadoop.fs.ftp.FTPFileSystem:getFileStatus
org.apache.hadoop.ipc.Server:getAuthMethods
org.apache.hadoop.http.HttpServer:getFilterInitializers
org.apache.hadoop.conf.Configuration:getInstances
org.apache.hadoop.conf.Configuration:getFile
org.apache.hadoop.conf.Configuration:getInts
org.apache.hadoop.conf.Configuration:getClasses
org.apache.hadoop.fs.s3native.NativeS3FileSystem:create
org.apache.hadoop.fs.s3native.NativeS3FileSystem$NativeS3FsOutputStream:<init>
org.apache.hadoop.ipc.Client$Connection$2:run
org.apache.hadoop.ipc.Client$Connection:access$1700
org.apache.hadoop.ipc.Client$Connection:setupSaslConnection
org.apache.hadoop.security.SaslRpcClient:saslConnect
org.apache.hadoop.security.SaslRpcClient:selectSaslClient
org.apache.hadoop.security.SaslRpcClient:createSaslClient
org.apache.hadoop.io.compress.DefaultCodec:createOutputStream
org.apache.hadoop.io.compress.DefaultCodec:createCompressor
org.apache.hadoop.io.compress.GzipCodec:createOutputStream
org.apache.hadoop.io.compress.GzipCodec:createCompressor
org.apache.hadoop.io.compress.GzipCodec$GzipZlibCompressor:<init>
org.apache.hadoop.io.compress.zlib.BuiltInZlibDeflater:reinit
org.apache.hadoop.io.compress.zlib.ZlibCompressor:reinit
org.apache.hadoop.io.compress.zlib.ZlibFactory:getZlibCompressor
org.apache.hadoop.io.compress.zlib.ZlibCompressor:<init>
org.apache.hadoop.io.compress.zlib.ZlibFactory:getCompressionLevel
org.apache.hadoop.io.compress.zlib.ZlibFactory:getCompressionStrategy
org.apache.hadoop.ipc.Server$Listener:<init>
org.apache.hadoop.ipc.Server:bind
org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore:initialize
org.apache.hadoop.fs.s3.Jets3tFileSystemStore:initialize
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:create
org.apache.hadoop.fs.DelegateToFileSystem:createInternal
org.apache.hadoop.fs.FilterFileSystem:primitiveCreate
org.apache.hadoop.fs.shell.Touch$Touchz:processNonexistentPath
org.apache.hadoop.fs.shell.Touch$Touchz:processPath
org.apache.hadoop.fs.RawLocalFileSystem:moveFromLocalFile
org.apache.hadoop.fs.LocalFileSystem:copyToLocalFile
org.apache.hadoop.fs.LocalFileSystem:copyFromLocalFile
org.apache.hadoop.fs.ChecksumFileSystem:copyFromLocalFile
org.apache.hadoop.fs.RawLocalFileSystem:rename
org.apache.hadoop.fs.HarFileSystem:copyToLocalFile
org.apache.hadoop.fs.ChecksumFileSystem:createNonRecursive
org.apache.hadoop.fs.ChecksumFileSystem:create
org.apache.hadoop.fs.FilterFileSystem:create
org.apache.hadoop.fs.FileSystem:createNewFile
org.apache.hadoop.fs.FileSystem:primitiveCreate
org.apache.hadoop.fs.shell.Touch$Touchz:touchz
org.apache.hadoop.fs.FileUtil:copyMerge
org.apache.hadoop.io.BloomMapFile$Writer:close
org.apache.hadoop.fs.shell.CopyCommands$AppendToFile:processArguments
org.apache.hadoop.fs.FileUtil:copy
org.apache.hadoop.fs.viewfs.ViewFileSystem:create
org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer:<init>
org.apache.hadoop.fs.shell.CopyCommands$Merge:processArguments
org.apache.hadoop.fs.FileSystem:create
org.apache.hadoop.io.DefaultStringifier:loadArray
org.apache.hadoop.io.SequenceFile$Reader:initialize
org.apache.hadoop.io.WritableUtils:clone
org.apache.hadoop.util.ReflectionUtils:copy
org.apache.hadoop.io.DefaultStringifier:<init>
org.apache.hadoop.io.SequenceFile$Reader:init
org.apache.hadoop.io.SequenceFile$Writer:init
org.apache.hadoop.util.ReflectionUtils:getFactory
org.apache.hadoop.io.serializer.avro.AvroReflectSerialization:accept
org.apache.hadoop.io.serializer.SerializationFactory:<init>
org.apache.hadoop.io.serializer.avro.AvroReflectSerialization:getPackages
org.apache.hadoop.io.BloomMapFile$Writer:append
org.apache.hadoop.util.bloom.DynamicBloomFilter:add
org.apache.hadoop.util.bloom.RetouchedBloomFilter:<init>
org.apache.hadoop.util.bloom.DynamicBloomFilter:addRow
org.apache.hadoop.util.bloom.DynamicBloomFilter:readFields
org.apache.hadoop.util.bloom.CountingBloomFilter:<init>
org.apache.hadoop.util.bloom.DynamicBloomFilter:<init>
org.apache.hadoop.util.bloom.BloomFilter:<init>
org.apache.hadoop.util.bloom.RetouchedBloomFilter:readFields
org.apache.hadoop.util.bloom.CountingBloomFilter:readFields
org.apache.hadoop.util.bloom.BloomFilter:readFields
org.apache.hadoop.util.bloom.Filter:<init>
org.apache.hadoop.util.bloom.Filter:readFields
org.apache.hadoop.util.bloom.HashFunction:<init>
org.apache.hadoop.io.BloomMapFile$Writer:initBloomFilter
org.apache.hadoop.util.hash.Hash:getInstance
org.apache.hadoop.fs.s3.MigrationTool:run
org.apache.hadoop.ha.SshFenceByTcpPort:tryFence
org.apache.hadoop.ha.SshFenceByTcpPort:createSession
org.apache.hadoop.ha.SshFenceByTcpPort:getKeyFiles
org.apache.hadoop.ipc.Server:refreshServiceAcl
org.apache.hadoop.io.nativeio.NativeIO:getOwner
org.apache.hadoop.fs.s3native.NativeS3FileSystem:mkdirs
org.apache.hadoop.fs.s3native.NativeS3FileSystem:rename
org.apache.hadoop.fs.s3native.NativeS3FileSystem:mkdir
org.apache.hadoop.fs.s3native.NativeS3FileSystem:delete
org.apache.hadoop.fs.s3native.NativeS3FileSystem:open
org.apache.hadoop.fs.s3native.NativeS3FileSystem:getFileStatus
org.apache.hadoop.fs.s3native.NativeS3FileSystem:listStatus
org.apache.hadoop.fs.s3native.NativeS3FileSystem:newFile
org.apache.hadoop.security.authorize.AccessControlList$1:newInstance
org.apache.hadoop.security.authorize.AccessControlList:<init>
org.apache.hadoop.security.Groups:getUserToGroupsMappingService
org.apache.hadoop.security.UserGroupInformation$TestingGroups:<init>
org.apache.hadoop.fs.LocalFileSystem:reportChecksumFailure
org.apache.hadoop.fs.DF:main
org.apache.hadoop.fs.s3native.NativeS3FileSystem:initialize
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getServerDefaults
org.apache.hadoop.fs.FilterFileSystem:getServerDefaults
org.apache.hadoop.fs.viewfs.ViewFileSystem:getServerDefaults
org.apache.hadoop.fs.DelegateToFileSystem:getServerDefaults
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getDefaultBlockSize
org.apache.hadoop.fs.viewfs.ViewFileSystem:getDefaultBlockSize
org.apache.hadoop.fs.FileSystem:getServerDefaults
org.apache.hadoop.fs.FilterFileSystem:getDefaultBlockSize
org.apache.hadoop.fs.s3.S3FileSystem:initialize
org.apache.hadoop.ha.ZKFailoverController:initHM
org.apache.hadoop.io.nativeio.NativeIO:ensureInitialized
org.apache.hadoop.fs.s3native.NativeS3FileSystem:getDefaultBlockSize
org.apache.hadoop.fs.s3.S3FileSystem:getDefaultBlockSize
org.apache.hadoop.io.nativeio.NativeIO$POSIX:<clinit>
org.apache.hadoop.security.Groups:<init>
org.apache.hadoop.fs.DF:<init>
org.apache.hadoop.fs.s3native.NativeS3FileSystem:createDefaultStore
org.apache.hadoop.fs.FileSystem:getDefaultBlockSize
org.apache.hadoop.fs.s3.S3FileSystem:createDefaultStore
org.apache.hadoop.ha.HealthMonitor:<init>
org.apache.hadoop.fs.TrashPolicyDefault$Emptier:run
org.apache.hadoop.fs.TrashPolicyDefault:<init>
org.apache.hadoop.fs.TrashPolicyDefault:initialize
org.apache.hadoop.fs.shell.Display$Cat:processPath
org.apache.hadoop.fs.HarFileSystem$HarMetaData:access$000
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:open
org.apache.hadoop.fs.shell.Tail:processPath
org.apache.hadoop.fs.HarFileSystem:open
org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream:<init>
org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:read
org.apache.hadoop.io.SequenceFile$Reader:openFile
org.apache.hadoop.fs.ChecksumFileSystem:open
org.apache.hadoop.fs.shell.Display$Cat:getInputStream
org.apache.hadoop.fs.viewfs.ViewFileSystem:open
org.apache.hadoop.fs.HarFileSystem$HarMetaData:parseMetaData
org.apache.hadoop.fs.FilterFileSystem:open
org.apache.hadoop.io.SequenceFile$Sorter$SegmentContainer:<init>
org.apache.hadoop.fs.shell.Tail:dumpFromOffset
org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:<init>
org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:<init>
org.apache.hadoop.fs.DelegateToFileSystem:open
org.apache.hadoop.io.compress.Lz4Codec:createOutputStream
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:append
org.apache.hadoop.fs.viewfs.ViewFileSystem:append
org.apache.hadoop.fs.FilterFileSystem:append
org.apache.hadoop.io.BloomMapFile$Reader:get
org.apache.hadoop.io.SetFile$Reader:get
org.apache.hadoop.io.MapFile$Reader:get
org.apache.hadoop.io.SetFile$Reader:seek
org.apache.hadoop.io.MapFile$Reader:seek
org.apache.hadoop.io.MapFile$Reader:getClosest
org.apache.hadoop.io.MapFile$Reader:midKey
org.apache.hadoop.io.MapFile$Reader:finalKey
org.apache.hadoop.io.MapFile$Reader:seekInternal
org.apache.hadoop.fs.shell.Display$TextRecordInputStream:read
org.apache.hadoop.io.MapFile$Reader:next
org.apache.hadoop.io.MapFile$Reader:readIndex
org.apache.hadoop.io.SequenceFile$Reader:next
org.apache.hadoop.io.SequenceFile$Reader:sync
org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:<init>
org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:reinit
org.apache.hadoop.io.SequenceFile:access$100
org.apache.hadoop.io.file.tfile.TFile$Reader:createScanner
org.apache.hadoop.io.file.tfile.TFile$Reader:createScannerByKey
org.apache.hadoop.io.file.tfile.TFile$Reader:createScannerByByteRange
org.apache.hadoop.io.file.tfile.TFile$Reader:createScannerByRecordNum
org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:upperBound
org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:lowerBound
org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:<init>
org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:rewind
org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:advance
org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:seekTo
org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:initBlock
org.apache.hadoop.io.file.tfile.TFile$Reader:getBlockReader
org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:getRecordNum
org.apache.hadoop.io.file.tfile.TFile$Reader:getRecordNumNear
org.apache.hadoop.io.file.tfile.TFile$Reader:getBlockContainsKey
org.apache.hadoop.io.file.tfile.TFile$Reader:getFirstKey
org.apache.hadoop.io.file.tfile.TFile$Reader:getLastKey
org.apache.hadoop.io.file.tfile.TFile$Reader:getKeyNear
org.apache.hadoop.io.file.tfile.TFile$Reader:getLocationByRecordNum
org.apache.hadoop.io.file.tfile.TFile$Reader:getRecordNumByLocation
org.apache.hadoop.io.file.tfile.TFile$Reader:<init>
org.apache.hadoop.io.file.tfile.TFile$Reader:checkTFileDataIndex
org.apache.hadoop.io.file.tfile.BCFile$Reader:<init>
org.apache.hadoop.io.file.tfile.TFile$Reader:getMetaBlock
org.apache.hadoop.io.file.tfile.BCFile$Reader:getDataBlock
org.apache.hadoop.io.file.tfile.BCFile$Reader:getMetaBlock
org.apache.hadoop.io.file.tfile.BCFile$Reader:createReader
org.apache.hadoop.io.file.tfile.BCFile$Reader$RBlockState:<init>
org.apache.hadoop.io.file.tfile.TFile$Writer:append
org.apache.hadoop.io.file.tfile.TFile$Writer:prepareAppendValue
org.apache.hadoop.ipc.Client:getTimeout
org.apache.hadoop.net.ScriptBasedMapping:<init>
org.apache.hadoop.net.ScriptBasedMapping:setConf
org.apache.hadoop.util.RunJar:unJar
org.apache.hadoop.fs.FileContext$Util:copy
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:writeStreamToFile
org.apache.hadoop.fs.shell.Display$Cat:printToStdout
org.apache.hadoop.fs.ChecksumFileSystem:access$000
org.apache.hadoop.io.file.tfile.TFile$Writer:prepareAppendKey
org.apache.hadoop.io.file.tfile.TFile$Writer:initDataBlock
org.apache.hadoop.io.file.tfile.TFile$Writer:close
org.apache.hadoop.io.file.tfile.BCFile$Writer:close
org.apache.hadoop.io.file.tfile.TFile$Writer:prepareMetaBlock
org.apache.hadoop.io.file.tfile.BCFile$Writer:prepareDataBlock
org.apache.hadoop.io.file.tfile.BCFile$Writer:prepareMetaBlock
org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState:<init>
org.apache.hadoop.fs.FileSystem:open
org.apache.hadoop.io.compress.Lz4Codec:createCompressor
org.apache.hadoop.io.SequenceFile$Sorter:<init>
org.apache.hadoop.fs.FileSystem:append
org.apache.hadoop.ha.FailoverController:getGracefulFenceTimeout
org.apache.hadoop.io.compress.SnappyCodec:createOutputStream
org.apache.hadoop.io.SequenceFile$Reader:handleChecksumException
org.apache.hadoop.io.compress.DefaultCodec:createInputStream
org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getBlockSize
org.apache.hadoop.ipc.Client$ConnectionId:getConnectionId
org.apache.hadoop.io.compress.SnappyCodec:createInputStream
org.apache.hadoop.util.LineReader:<init>
org.apache.hadoop.io.SequenceFile:getBufferSize
org.apache.hadoop.fs.ChecksumFileSystem:setConf
org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getWorkFactor
org.apache.hadoop.io.file.tfile.TFile:getFSInputBufferSize
org.apache.hadoop.fs.HarFileSystem:initializeMetadataCache
org.apache.hadoop.io.compress.SnappyCodec:createDecompressor
org.apache.hadoop.io.file.tfile.TFile:getChunkBufferSize
org.apache.hadoop.ipc.Client:getPingInterval
org.apache.hadoop.ha.SshFenceByTcpPort:getSshConnectTimeout
org.apache.hadoop.io.compress.SnappyCodec:createCompressor
org.apache.hadoop.io.compress.Lz4Codec:createInputStream
org.apache.hadoop.ha.FailoverController:getRpcTimeoutToNewActive
org.apache.hadoop.io.compress.Lz4Codec:createDecompressor
org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:setConf
org.apache.hadoop.ha.HAAdmin:setConf
org.apache.hadoop.io.IOUtils:copyBytes
org.apache.hadoop.fs.ChecksumFileSystem:getSumBufferSize
org.apache.hadoop.io.compress.GzipCodec:createInputStream
org.apache.hadoop.io.file.tfile.TFile:getFSOutputBufferSize
org.apache.hadoop.fs.FileContext$2:run
org.apache.hadoop.fs.AbstractFileSystem:get
org.apache.hadoop.fs.viewfs.ViewFs$1:getTargetFileSystem
org.apache.hadoop.fs.FileSystem$Cache:getInternal
org.apache.hadoop.fs.FileSystem:access$200
org.apache.hadoop.fs.FsUrlStreamHandlerFactory:createURLStreamHandler
org.apache.hadoop.fs.FileSystem:createFileSystem
org.apache.hadoop.fs.FsUrlStreamHandlerFactory:<init>
org.apache.hadoop.ipc.RPC$Builder:build
org.apache.hadoop.fs.AbstractFileSystem:createFileSystem
org.apache.hadoop.fs.FileSystem:getFileSystemClass
org.apache.hadoop.net.NetworkTopology:getInstance
org.apache.hadoop.ipc.RPC:getProtocolEngine
org.apache.hadoop.fs.TrashPolicy:getInstance
org.apache.hadoop.fs.FileContext:createSymlink
org.apache.hadoop.io.compress.DefaultCodec:getCompressorType
org.apache.hadoop.io.compress.DefaultCodec:getDecompressorType
org.apache.hadoop.io.compress.DefaultCodec:createDecompressor
org.apache.hadoop.io.compress.zlib.ZlibFactory:getZlibCompressorType
org.apache.hadoop.io.compress.zlib.ZlibFactory:getZlibDecompressorType
org.apache.hadoop.io.compress.GzipCodec:getCompressorType
org.apache.hadoop.io.compress.GzipCodec:getDecompressorType
org.apache.hadoop.io.compress.zlib.ZlibFactory:getZlibDecompressor
org.apache.hadoop.io.compress.GzipCodec:createDecompressor
org.apache.hadoop.fs.local.LocalFs:<init>
org.apache.hadoop.fs.local.RawLocalFs:<init>
org.apache.hadoop.fs.ftp.FtpFs:<init>
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:initialize
org.apache.hadoop.fs.DelegateToFileSystem:<init>
org.apache.hadoop.fs.FilterFileSystem:initialize
org.apache.hadoop.fs.LocalFileSystem:initialize
org.apache.hadoop.fs.RawLocalFileSystem:initialize
org.apache.hadoop.http.HttpConfig:<clinit>
org.apache.hadoop.fs.FileSystem:isSymlinksEnabled
org.apache.hadoop.io.compress.zlib.ZlibFactory:isNativeZlibLoaded
org.apache.hadoop.http.HttpServer:addDefaultApps
org.apache.hadoop.util.NativeCodeLoader:getLoadNativeLibraries
org.apache.hadoop.fs.FileSystem:initialize
org.apache.hadoop.conf.Configuration:getLong
org.apache.hadoop.conf.Configuration:getLongBytes
org.apache.hadoop.conf.Configuration:getFloat
org.apache.hadoop.conf.Configuration:getDouble
org.apache.hadoop.conf.Configuration:getInt
org.apache.hadoop.conf.Configuration:getClass
org.apache.hadoop.conf.Configuration:getBoolean
org.apache.hadoop.fs.s3.S3FileSystem:create
org.apache.hadoop.fs.s3.S3OutputStream:close
org.apache.hadoop.fs.s3.S3OutputStream:write
org.apache.hadoop.fs.s3.S3OutputStream:flush
org.apache.hadoop.fs.s3.S3OutputStream:<init>
org.apache.hadoop.fs.s3.S3OutputStream:endBlock
org.apache.hadoop.security.ssl.SSLFactory:init
org.apache.hadoop.security.ssl.SSLFactory:configure
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getContentSummary
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:rename
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getFileStatus
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getFileChecksum
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:listStatus
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setPermission
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setReplication
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:delete
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:resolvePath
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setOwner
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setTimes
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:createNonRecursive
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getStatus
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getDefaultReplication
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getFileBlockLocations
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:mkdirs
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:fullPath
org.apache.hadoop.fs.viewfs.ViewFileSystem:resolvePath
org.apache.hadoop.fs.FilterFileSystem:resolvePath
org.apache.hadoop.fs.DelegateToFileSystem:getHomeDirectory
org.apache.hadoop.fs.FilterFileSystem:getHomeDirectory
org.apache.hadoop.fs.viewfs.ViewFileSystem:getTrashCanLocation
org.apache.hadoop.fs.FileSystem:getHomeDirectory
org.apache.hadoop.fs.TrashPolicyDefault:moveToTrash
org.apache.hadoop.fs.FilterFileSystem:makeQualified
org.apache.hadoop.fs.FilterFileSystem:checkPath
org.apache.hadoop.fs.FileSystem:resolvePath
org.apache.hadoop.fs.FileSystem:makeQualified
org.apache.hadoop.fs.FileSystem:checkPath
org.apache.hadoop.fs.HarFileSystem:decodeHarURI
org.apache.hadoop.conf.ConfServlet:writeResponse
org.apache.hadoop.io.DefaultStringifier:load
org.apache.hadoop.security.LdapGroupsMapping:setConf
org.apache.hadoop.io.retry.RetryUtils:getMultipleLinearRandomRetry
org.apache.hadoop.ha.NodeFencer:create
org.apache.hadoop.fs.viewfs.ConfigUtil:getHomeDirValue
org.apache.hadoop.io.SequenceFile:getDefaultCompressionType
org.apache.hadoop.io.compress.bzip2.Bzip2Factory:isNativeBzip2Loaded
org.apache.hadoop.ha.ZKFailoverController:initZK
org.apache.hadoop.ha.ZKFailoverController:getParentZnode
org.apache.hadoop.net.TableMapping$RawTableMapping:load
org.apache.hadoop.io.file.tfile.Compression$Algorithm$1:isSupported
org.apache.hadoop.util.RunJar:main
org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:confChanged
org.apache.hadoop.http.lib.StaticUserWebFilter:getUsernameFromConf
org.apache.hadoop.fs.s3.Jets3tFileSystemStore:newBackupFile
org.apache.hadoop.net.NetUtils:getDefaultSocketFactory
org.apache.hadoop.security.HadoopKerberosName:setConfiguration
org.apache.hadoop.io.compress.CompressionCodecFactory:getCodecClasses
org.apache.hadoop.fs.ftp.FTPFileSystem:connect
org.apache.hadoop.conf.Configuration:getSocketAddr
org.apache.hadoop.security.SecurityUtil:getAuthenticationMethod
org.apache.hadoop.net.AbstractDNSToSwitchMapping:isSingleSwitchByScriptPolicy
org.apache.hadoop.conf.Configuration:getTrimmedStrings
org.apache.hadoop.net.SocksSocketFactory:setConf
org.apache.hadoop.conf.Configuration:getPattern
org.apache.hadoop.fs.s3native.NativeS3FileSystem$NativeS3FsOutputStream:newBackupFile
org.apache.hadoop.http.HttpServer:addSslListener
org.apache.hadoop.security.SaslRpcClient:getServerPrincipal
org.apache.hadoop.conf.Configuration:getEnum
org.apache.hadoop.conf.Configuration:getRange
org.apache.hadoop.fs.s3.S3Credentials:initialize
org.apache.hadoop.fs.permission.FsPermission:getUMask
org.apache.hadoop.conf.Configuration:getTimeDuration
org.apache.hadoop.conf.Configuration:getStrings
org.apache.hadoop.util.hash.Hash:getHashType
org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:init
org.apache.hadoop.fs.s3.MigrationTool:initialize
org.apache.hadoop.security.SaslRpcServer:init
org.apache.hadoop.conf.Configuration:getTrimmedStringCollection
org.apache.hadoop.http.HttpServer:initSpnego
org.apache.hadoop.security.authorize.ServiceAuthorizationManager:refresh
org.apache.hadoop.metrics.ganglia.GangliaContext31:init
org.apache.hadoop.conf.Configuration:getStringCollection
org.apache.hadoop.conf.Configuration:getTrimmed
org.apache.hadoop.fs.s3.S3OutputStream:newBackupFile
org.apache.hadoop.security.ssl.SSLFactory:getHostnameVerifier
org.apache.hadoop.fs.FileSystem:getDefaultUri
org.apache.hadoop.conf.Configuration:dumpConfiguration
org.apache.hadoop.security.authorize.ProxyUsers:refreshSuperUserGroupsConfiguration
org.apache.hadoop.conf.ReconfigurationServlet:doPost
org.apache.hadoop.conf.Configuration:substituteVars
org.apache.hadoop.conf.ReconfigurationServlet:applyChanges
org.apache.hadoop.conf.Configuration:main
org.apache.hadoop.conf.Configuration:writeXml
org.apache.hadoop.conf.Configuration:asXmlDocument
org.apache.hadoop.conf.Configuration:set
org.apache.hadoop.conf.Configuration:iterator
org.apache.hadoop.conf.Configuration:get
org.apache.hadoop.conf.Configuration:getValByRegex
org.apache.hadoop.conf.Configuration:getRaw
org.apache.hadoop.conf.Configuration:write
org.apache.hadoop.conf.Configuration:size
org.apache.hadoop.conf.Configuration:clear
org.apache.hadoop.conf.Configuration:unset
org.apache.hadoop.conf.Configuration:getPropertySources
org.apache.hadoop.conf.Configuration:handleDeprecation
org.apache.hadoop.conf.Configuration:getProps
org.apache.hadoop.conf.Configuration:loadResources
org.apache.hadoop.conf.Configuration:loadResource
org.apache.hadoop.conf.Configuration:parse
